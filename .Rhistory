facet_grid(credit_history~purpose)
credit %>%
count(default)
credit %>%
ggplot(aes(employment_duration, amount)) +
geom_point(aes(color = default))
credit %>%
ggplot(aes(employment_duration, amount)) +
geom_boxplot(aes(color = default))
credit %>%
count(employment_duration)
sum("a")
try(sum("a"))
foo <- try(sum("a"))
foo
?try
{
print('a')
}
if(TRUE)
{
print('a')
foo <- try(sum("a"))
print('b')
}
if(TRUE)
{
print('a')
try(sum("a"))
print('b')
}
if(TRUE)
{
print('a')
sum("a")
print('b')
}
try(sum("a"), silent = TRUE)
{
print('a')
try(sum("a"))
print('b')
}
{
print('a')
sum("a")
print('b')
}
sum("a")
try(sum("a"), silent = TRUE)
{
print('a')
try(sum("a"))
print('b')
}
{
print('a')
sum("a")
print('b')
}
foo <-  try(sum("a"), silent = TRUE)
foo[1]
foo[[1]]
foo[[2]]
foo[2]
foo
class(foo)
class(try(sum("a"), silent = TRUE))
sum(1)
class(try(sum(1), silent = TRUE))
CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
# cross tabulation of predicted versus actual classes
library(gmodels)
CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
credit_pred
credit_pred <- predict(credit_model, credit_test)
credit_model
credit_test
credit <- read.csv("source/Chapter05/credit.csv", stringsAsFactors = TRUE)
library(C50)       # деревья решений
library(gmodels)   # оценка эффективности
library(tidyverse) # инструменты
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample, ]
credit_test  <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
credit_model <- C5.0(credit_train[-which(colnames(credit_train) == "default")],
factor(credit_train$default))
credit_model <- C5.0(credit_train[-which(colnames(credit_train) == "default")],
credit_train$default)
credit_pred <- predict(credit_model, credit_test)
CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
cross_table <- CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default')) %>%
as.data.frame()
cross_table %>%
as_tibble() %>%
mutate(check = t.x == t.y) %>%
group_by(check) %>%
summarise(check_sum = sum(t.Freq)) %>%
mutate(result = check_sum/sum(check_sum)) %>%
filter(check == TRUE) %>%
pull()
credit_boost10 <- C5.0(credit_train[-17], credit_train$default,
trials = 10)
credit_boost_pred10 <- predict(credit_boost10, credit_test)
predict
credit_boost_pred10 <- predict(credit_boost10, credit_test)
credit_boost_pred10
CrossTable(credit_test$default, credit_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
cross_table2 <- CrossTable(credit_test$default, credit_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default')) %>%
as.data.frame()
cross_table2 %>%
as_tibble() %>%
mutate(check = t.x == t.y) %>%
group_by(check) %>%
summarise(check_sum = sum(t.Freq)) %>%
mutate(result = check_sum/sum(check_sum)) %>%
filter(check == TRUE) %>%
pull()
rnorm(1:10)
set.seed(42)
rnorm(1:10)
set.seed(42)
rnorm(1:10)
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
matrix_dimensions
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
credit_cost <- C5.0(credit_train[-17], credit_train$default,
costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
28/35
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
mushrooms <- read.csv("source/Chapter05/mushrooms.csv", stringsAsFactors = TRUE)
mushrooms
mushrooms %>% glimpse()
mushrooms
mushrooms %>%
as_tibble()
mushrooms %>%
as_tibble() %>% summary()
mushrooms$veil_type <- NULL
table(mushrooms$type)
mushroom_1R <- OneR(type ~ ., data = mushrooms)
## Step 3: Training a model on the data ----
library(OneR)
mushroom_1R <- OneR(type ~ ., data = mushrooms)
mushroom_1R
mushroom_1R_pred <- predict(mushroom_1R, mushrooms)
table(actual = mushrooms$type, predicted = mushroom_1R_pred)
## Step 5: Improving model performance ----
library(RWeka)
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
summary(mushroom_JRip)
mushroom_c5rules <- C5.0(type ~ odor + gill_size, data = mushrooms, rules = TRUE)
summary(mushroom_c5rules)
mushroom_c5rules <- C50::C5.0(type ~ odor + gill_size, data = mushrooms, rules = TRUE)
summary(mushroom_c5rules)
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
launch <- readюcsv("source/Chapter06/challenger.csv")
launch <- read.csv("source/Chapter06/challenger.csv")
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b
a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a
r <- cov(launch$temperature, launch$distress_ct) /
(sd(launch$temperature) * sd(launch$distress_ct))
r
# либо уже готовой функцией:
cor(launch$temperature, launch$distress_ct)
r * (sd(launch$distress_ct) / sd(launch$temperature))
b
r * (sd(launch$distress_ct) / sd(launch$temperature))
b
# confirming the regression line using the lm function (not in text)
model <- lm(distress_ct ~ temperature, data = launch)
model
summary(model)
reg <- function(y, x) {
x <- as.matrix(x)
x <- cbind(Intercept = 1, x)
b <- solve(t(x) %*% x) %*% t(x) %*% y
colnames(b) <- "estimate"
print(b)
}
str(launch)
reg(y = launch$distress_ct, x = launch[2])
reg(y = launch$distress_ct, x = launch[2:4])
launch$distress_ct
launch[2]
launch[2:4]
reg(y = launch$distress_ct, x = launch[2:4])
model <- lm(distress_ct ~ temperature + field_check_pressure + flight_num, data = launch)
model
# use regression model with multiple regression
reg(y = launch$distress_ct, x = launch[2:4])
insurance <- read.csv("source/Chapter06/insurance.csv", stringsAsFactors = TRUE)
str(insurance)
summary(insurance$expenses)
hist(insurance$expenses)
summary(insurance)
cor(insurance[c("age", "bmi", "children", "expenses")])
pairs(insurance[c("age", "bmi", "children", "expenses")])
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
ins_model <- lm(expenses ~ ., data = insurance) # this is equivalent to above
ins_model
summary(ins_model)
insurance$age2 <- insurance$age^2
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
bmi30*smoker + region, data = insurance)
summary(ins_model2)
cor(insurance$pred, insurance$expenses)
insurance$pred <- predict(ins_model2, insurance)
cor(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)
plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)
predict(ins_model2,
data.frame(age = 30, age2 = 30^2, children = 2,
bmi = 30, sex = "male", bmi30 = 1,
smoker = "no", region = "northeast"))
predict(ins_model2,
data.frame(age = 30, age2 = 30^2, children = 2,
bmi = 30, sex = "female", bmi30 = 1,
smoker = "no", region = "northeast"))
predict(ins_model2,
data.frame(age = 30, age2 = 30^2, children = 0,
bmi = 30, sex = "female", bmi30 = 1,
smoker = "no", region = "northeast"))
ins_model2
predict(ins_model2,
data.frame(age = 30, age2 = 30^2, children = 0,
bmi = 30, sex = "female", bmi30 = 1,
smoker = "no", region = "northeast"))
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
# compute the SDR
sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) + length(at2) / length(tee) * sd(at2))
sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) + length(bt2) / length(tee) * sd(bt2))
# compare the SDR for each split
sdr_a
sdr_b
insurance <- read.csv("source/Chapter06/whitewines.csv")
insurance
insurance %>% glimpse()
library(tidyverse)
insurance %>% glimpse()
hist(wine$quality)
wine <- read.csv("source/Chapter06/whitewines.csv")
wine %>% glimpse()
hist(wine$quality)
wine$quality
wine
wine %>%
ggplot(aes(quality)) +
geom_histogram()
wine %>%
ggplot(aes(quality)) +
#geom_histogram()
geom_boxplot()
wine %>%
count(quality)
# summary statistics of the wine data
summary(wine)
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
## Step 3: Training a model on the data ----
# regression tree using rpart
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
# get more detailed information about the tree
summary(m.rpart)
rpart.plot(m.rpart, digits = 3)
(rpart.plot)
library(rpart.plot)
rpart.plot(m.rpart, digits = 3)
# a few adjustments to the diagram
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
p.rpart <- predict(m.rpart, wine_test)
p.rpart
# compare the distribution of predicted values vs. actual values
summary(p.rpart)
summary(wine_test$quality)
wine <- read.csv("source/Chapter06/whitewines.csv")
hist(wine$quality)
wine %>%
count(quality)
wine %>%
ggplot(aes(quality)) +
#geom_histogram()
geom_boxplot()
library(tidyverse)
wine %>%
ggplot(aes(quality)) +
#geom_histogram()
geom_boxplot()
wine %>%
count(quality)
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)
summary(m.rpart)
library(rpart.plot)
rpart.plot(m.rpart, digits = 3)
p.rpart <- predict(m.rpart, wine_test)
install.packages("Cubist")
library(Cubist)
m.cubist <- cubist(x = wine_train[-12], y = wine_train$quality)
# display basic information about the model tree
m.cubist
# display the tree itself
summary(m.cubist)
# generate predictions for the model
p.cubist <- predict(m.cubist, wine_test)
# summary statistics about the predictions
summary(p.cubist)
# correlation between the predicted and true values
cor(p.cubist, wine_test$quality)
# mean absolute error of predicted and true values
# (uses a custom function defined above)
MAE(wine_test$quality, p.cubist)
library(rpart)
# mean absolute error of predicted and true values
# (uses a custom function defined above)
MAE(wine_test$quality, p.cubist)
# more informative scatterplot matrix
library(psych)
MAE(wine_test$quality, p.cubist)
# use the rpart.plot package to create a visualization
library(rpart.plot)
MAE(wine_test$quality, p.cubist)
# функция средней абсолютной ошибки
MAE <- function(actual, predicted) {
mean(abs(actual - predicted))
}
MAE(wine_test$quality, p.cubist)
wine <- read.csv("source/Chapter07/concrete.csv")
rm(wine)
concrete %>% glimpse()
concrete <- read.csv("source/Chapter07/concrete.csv")
concrete %>% glimpse()
concrete
concrete %>%
pivot_longer(cols = c(-strength),
names_to = "feature",
values_to = "value")
concrete %>%
pivot_longer(cols = everything(),
names_to = "feature",
values_to = "value")
concrete %>%
pivot_longer(cols = everything(),
names_to = "feature",
values_to = "value") %>%
ggplot(aes(value)) +
geom_histogram() +
facet_wrap(~feature)
concrete %>%
pivot_longer(cols = everything(),
names_to = "feature",
values_to = "value") %>%
ggplot(aes(value)) +
geom_histogram() +
facet_wrap(~feature, scales = "free")
# мини-максная нормализация
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
concrete_norm
concrete_norm %>% as_tibble()
?map
map(concrete, normalize)
map(concrete, normalize) %>%
as.data.frame()
map(concrete, normalize) %>% as_tibble()
concrete_norm
map(concrete, normalize) %>% as_tibble()
concrete_norm %>% as_tibble()
concrete_norm %>% summary()
library(tidymodels)
install.packages("tidymodels")
library(tidymodels)
mario_train <- training(mario_split)
mario_train <- training(concrete_norm)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
install.packages("neuralnet")
library(neuralnet)
?neuralnet
concrete_model <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train,
hidden = 1)
plot(concrete_model)
set.seed(12345) # to guarantee repeatable results
concrete_model <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train,
hidden = 1)
plot(concrete_model)
model_results <- compute(concrete_model, concrete_test[1:8])
predicted_strength <- model_results$net.result
predicted_strength
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train,
hidden = 5)
set.seed(12345) # to guarantee repeatable results
concrete_model2 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train,
hidden = 5)
plot(concrete_model2)
# evaluate the results as we did before
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
softplus <- function(x) { log(1 + exp(x)) }
set.seed(12345) # to guarantee repeatable results
concrete_model3 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = c(5, 5), act.fct = softplus)
# plot the network
plot(concrete_model3)
# evaluate the results as we did before
model_results3 <- compute(concrete_model3, concrete_test[1:8])
predicted_strength3 <- model_results3$net.result
cor(predicted_strength3, concrete_test$strength)
# note that the predicted and actual values are on different scales
strengths <- data.frame(
actual = concrete$strength[774:1030],
pred = predicted_strength3
)
head(strengths, n = 3)
# this doesn't change the correlations (but would affect absolute error)
cor(strengths$pred, strengths$actual)
# create an unnormalize function to reverse the normalization
unnormalize <- function(x) {
return((x * (max(concrete$strength)) -
min(concrete$strength)) + min(concrete$strength))
}
strengths$pred_new <- unnormalize(strengths$pred)
strengths$error <- strengths$pred_new - strengths$actual
head(strengths, n = 3)
cor(strengths$pred_new, strengths$actual)
letters <- read.csv("source/Chapter07/letterdata.csv", stringsAsFactors = TRUE)
str(letters)
letters
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train,
kernel = "vanilladot")
?ksvm
# look at basic information about the model
letter_classifier
letter_predictions <- predict(letter_classifier, letters_test)
table(letter_predictions, letters_test$letter)
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
set.seed(12345)
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
cost_values <- c(1, seq(from = 5, to = 40, by = 5))
accuracy_values <- sapply(cost_values, function(x) {
set.seed(12345)
m <- ksvm(letter ~ ., data = letters_train,
kernel = "rbfdot", C = x)
pred <- predict(m, letters_test)
agree <- ifelse(pred == letters_test$letter, 1, 0)
accuracy <- sum(agree) / nrow(letters_test)
return (accuracy)
})
plot(cost_values, accuracy_values, type = "b")
