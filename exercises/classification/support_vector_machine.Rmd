# Глава 7. Часть 2. Метод опорных векторов

В SVM методах строится граница, назваемая гиперплоскотью, для разделения данных на группы с одинаковыми значениями классов.

Плоскость которая лучше всего разделяет два класса --- оптимально разделяющая гиперплоскость (MMH)

Опорные вектора --- являетсяточками из каждого класса ближайшими к MMH. Каждый класс должен иметь хотя бы один опорный вектор, но их может быть и больше

В случае линено разделимых данных MMH находится как срединный перпендикуляр самого короткого отрезка соединяющего две выпуклые оболочки.

В случаях линейно неразделимых данных используется ослабляющая переменная, которая создаёт мягкую границу позволяющую некоторым точкам попадать не на свою стороную. Для каждой такой точке применяется штраф. И вместо того чтобы искать оатимально разделяющую плоскость, алгоритм старается минимизировать общий штраф.

Чем больше штраф, тем хуже алгоритм разделяет данные.

Трюк с ядрами --- добавление ещё одной переменной.

Бывают ядра --- линейные, полиномиальные, сигмоидные, Гауссово-RBF ядро.

# Шаг 1. Сбор данных


Данные --- рукописные буквы, преобразованные в глифы.

Что хотим --- различать рукописный текст.

```{r}
letters <- read.csv("source/Chapter07/letterdata.csv", stringsAsFactors = TRUE)
str(letters)
```

# Шаг 2. Исследование и подготовка данных

Данные уже приведены в форму подходяющую для построения моделей. КРоме того, данные уже рандомизированы, так что можно тестовые и обучающие выборки сформировать довольно просто.


```{r}
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
```


# Шаг 3. Обучение модели

```{r}
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train,
                          kernel = "vanilladot")

# vanilladot --- линейная функция
# другие по справке функции kvsm()



# look at basic information about the model
letter_classifier
```

# Шаг 4. Оценка эффективности модели

Получим предсказания

```{r}
letter_predictions <- predict(letter_classifier, letters_test)
```

Теперь сравним, предсказания и реальность. Сначала игрушечено, построково:

```{r}
table(letter_predictions, letters_test$letter)
```

Теперь общая оценка, которая не учитывает тип ошибки:

```{r}
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
```

точность получилась около 84%, что уже неплохо, но можно лучше.

# Шаг 5. Повышение эффективности модели

## Изменение функции ядра

Начинать все советуют с RBF-ядра, так и поступим

```{r}
set.seed(12345)
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
```

Проверим теперь качество полученной модели.

```{r}
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
```


## Изменение значение штрафа

ПОпробуем разные значения штрафа и посомтрим, как будет изменятся точность модели

```{r}
cost_values <- c(1, seq(from = 5, to = 40, by = 5))


accuracy_values <- sapply(cost_values, function(x) {
  set.seed(12345)
  m <- ksvm(letter ~ ., data = letters_train,
            kernel = "rbfdot", C = x)
  pred <- predict(m, letters_test)
  agree <- ifelse(pred == letters_test$letter, 1, 0)
  accuracy <- sum(agree) / nrow(letters_test)
  return (accuracy)
})

```


```{r}
plot(cost_values, accuracy_values, type = "b")
```

Общий подход ясен --- тренируем модели экспериментируя со штрафами и функциями ядер.