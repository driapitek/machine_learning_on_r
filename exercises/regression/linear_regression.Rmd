# Глава 5: Часть 1. Rule Learners

Линейная регрессия

## Справка

### Простая линейная регрессия

Формула ростейшей линейной регрессии выглядит как формула линейной функции

Коэффициенты линейной регрессии можно посчитать следующим образом

```{r}
launch <- read.csv("source/Chapter06/challenger.csv")

b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b

a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a

```

Так же важной характеристикой является корреляция, она считается следующим образом

```{r}
r <- cov(launch$temperature, launch$distress_ct) /
       (sd(launch$temperature) * sd(launch$distress_ct))
r

# либо уже готовой функцией:
cor(launch$temperature, launch$distress_ct)

# computing the slope using correlation
r * (sd(launch$distress_ct) / sd(launch$temperature))
```

Зная корреляцию можно посчитать slope или коэффициент b

```{r}
b
r * (sd(launch$distress_ct) / sd(launch$temperature))
```

Это здесь просто как напоминание. Вручную считать это не самая лучшая идея.

### Множественная регрессия

Как правило в реальном анализе есть несколько независимых переменных.

Цель множественной регрессии найти значения коэффициентов, которые связывают значения независимых переменных (x_i) от зависимой переменной (y), при минимальном значении ошибки (eps)

### Матричный поиск вектора коэффициентов Beta

В общем виде, с использованием матричной нотации поиск вектора выглядит следующим образом

$$\hat{\beta} = (X^T X)^{-1}X^TY$$
Исходя из этого напишем свою функцию для поиска коэффициента. Она принимает параметры X и Y, и возвращает вектор оценочных beta коэффициентов

```{r}
reg <- function(y, x) {
  x <- as.matrix(x)                       # переводим df в матрицу
  x <- cbind(Intercept = 1, x)            # добавляем столбец для $beta_0$
  b <- solve(t(x) %*% x) %*% t(x) %*% y   # считаем вектор $beta$
  colnames(b) <- "estimate"
  print(b)
}
```

```{r}
str(launch)

reg(y = launch$distress_ct, x = launch[2])

reg(y = launch$distress_ct, x = launch[2:4])
```

Проверим как работает наша функция, на примере встроенной функции `lm()`

```{r}
model <- lm(distress_ct ~ temperature + field_check_pressure + flight_num, data = launch)
model
```



## Шаг 1. Сбор данных

```{r}
library(tidyverse) # инструменты
library(psych)     # регрессионные модели
```  

```{r}
insurance <- read.csv("source/Chapter06/insurance.csv", stringsAsFactors = TRUE)
```

Данные --- расходы страховых компаний

Что хотим --- предсказать средние расходы на медицинское обслуживание

Немного о данных:

  * age --- возраст бенифициара
  * sex --- пол
  * bmi --- индекс массы дела
  * children --- число детей, на которых распространяется действие страховки
  * smoker --- курит или нет застрахованное лицо
  * region --- место жительства
  * expenses ---  расходы на мед.обслуживание

## Шаг 2. Исследование и подготовка данных

```{r}
str(insurance)

summary(insurance)
```

Зависимая переменная у нас это expenses --- затраты на расходы. Её мы и попытаемся предсказывать. Посмотрим на распределение данных, полезно бы проверить на нормальность.

```{r}
summary(insurance$expenses)
```

Среднее значение больше медианы, это значит что распределение скошено вправо.

```{r}
hist(insurance$expenses)
```

То что распределение не нормально, на самом деле проблема для регрессионной модели, но позже мы постараемся это решить.

Сейчас обратим внимаение на ещё одну --- регрессия работает только с числовыми данными, Но у нас есть категориальная:


### Шаг 2.1 матрица корреляции

Прежде чем строить модель, полезно узнать как связаны между собой переменные:

```{r}
cor(insurance[c("age", "bmi", "children", "expenses")])
```

Полезно посмотреть на матрицы рассеяния (SPLOM)

```{r}
pairs(insurance[c("age", "bmi", "children", "expenses")])
```

Более информативная матрица рассеяния

```{r}
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
```

## Шаг 3. Обучение модели

На готовых данных модель обучается легко:

```{r}
ins_model <- lm(expenses ~ ., data = insurance)
```

Объясним смысл этих коэффициентов:

  * Intercept --- это прогнозируемое значение затрат, когда все коэффы равны нулю. Во многих случаю сам сдвиг имеет мало смысла --- поскольку зачастую не бывает ситуаций когда все кэффы равны нулю. В нашем случае не бывает людей с нулевым возрастом и коэффом BMI.
  * Бэта-коэффы указывают на предполагаемое увеличение расходов, при увеличении признака на удиницу, при условии что все остальные кэффы остаются постоянными. Например ожидается, что расходы в среднем будут увеличиваться на 256,8 долларов, при условии что все остальные не изменяются.
  * По поводу новых переменных. R автоматом создаёт дамми-переменные. Что это означает. Когда добавляется фиктивная переменная, всегда есть эталонная. Остальные оценки интерпритируются относительно эталонной. В нашей модели эталон это --- sexfemale, smokerno, regionnortheast --- т.е. некурящие женщины с северо-востока. Следовательно у мужчин ежегодные расходы меньше на 131,4 доллара, а курильщики тратят больше некурящих на 23847,5 долларов больше.
  
## Шаг 4. Определение эффективности модели

```{r}
summary(ins_model)
```

Чем выше  R-squared тем лучше.

## Шаг 5. Повышение эффективности модели

### Добавление нелинейных зависимостей.

```{r}
insurance$age2 <- insurance$age^2
```

### Преобразование числовой переменной в двоичный фактор

Если вы не уверены, стоит ли включать переменную в модель, обычно практикой является включить её и проверить p-value. Если переменная не является статистически значимой, то у вас появятся доказательства, дающие возможность впоследствии её исключить.

```{r}
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

### Добавление эффектов взаимодействия

Что если отдельные признаки оказывают совместное взаимодействие на зависимую переменную? Например курение и ожирение могут оказывать влияние по отдельности, но разумно предположить, что их совокупный эффект может оказаться хуже, чем от каждого из них по отдельности. Это делается оператором `*`

```{r}
lm(expenses ~ bmi * smoker, data = insurance)
```


### Собираем воедино

Что мы сделали?

  * Добавили нелинейную переменную для возраста
  * создали индикатор ожирения
  * ввели зависимость между ожирение и курением

```{r}
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
                   bmi30*smoker + region, data = insurance)

summary(ins_model2)
```

Эффективность повысилась. Здорово!

Но сторого говоря, регрессионное моделирование делает сильные предположения о данных. Для числового прогнозирвоания эти допущения не сильно важны, поскольку ценность модели не основанна на том, действительно ли она отражает базоваый процесс, --- нас просто волнует точность прогнозов. Но если вы хотите сделать точные выводы на основании коээфов регрессионной модели, необходимо выполнить диагностические тесты, чтобы убедиться, что допущения регрессионной модели не были нарушены.

### Прогнозирование

Добавим прогнозы, чтобы оценить результаты

```{r}
insurance$pred <- predict(ins_model2, insurance)
```

Проверить качество, можно по тому на сколько прогнозы коррелируют с реальными значениями:

```{r}
cor(insurance$pred, insurance$expenses)
```

0.93 это неплохо.

```{r}

plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)
```

Ну и некоторые прогнозы нашей модели:
```{r}
predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 2,
                   bmi = 30, sex = "male", bmi30 = 1,
                   smoker = "no", region = "northeast"))

predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 2,
                   bmi = 30, sex = "female", bmi30 = 1,
                   smoker = "no", region = "northeast"))

predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 0,
                   bmi = 30, sex = "female", bmi30 = 1,
                   smoker = "no", region = "northeast"))
```

Коэффициенты можно экспортировать
